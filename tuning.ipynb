{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from data_generation import DenseGenerator, ChessPositionGen\n",
    "import keras_tuner as kt\n",
    "\n",
    "import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the same setup as in `full-puzzle-model.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paramaters on early stopping\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=5,\n",
    "                          verbose=1,\n",
    "                          mode='min',\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "log_dir = \"logs/fit/tuned\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Memory management, likely not necessary, but used as a safety as per the documentation recommendations on using GPUS\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('fens/train.csv')\n",
    "val = pd.read_csv('fens/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dense_gen = DenseGenerator(train, batch_size=1024)\n",
    "val_dense_gen = DenseGenerator(val, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tune_gen = ChessPositionGen(train, batch_size=512)\n",
    "val_tune_gen = ChessPositionGen(val, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron model for comparison with the CNN versions\n",
    "\n",
    "This model uses basic densely connected layers and uses a slight variation of the previously used data generator with no reshaping of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 832)               693056    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                53312     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 750,593\n",
      "Trainable params: 750,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "1977/1977 [==============================] - 2590s 1s/step - loss: 0.4226 - acc: 0.8320 - val_loss: 0.3911 - val_acc: 0.8487\n",
      "Epoch 2/15\n",
      "1977/1977 [==============================] - 2598s 1s/step - loss: 0.3766 - acc: 0.8544 - val_loss: 0.3697 - val_acc: 0.8579\n",
      "Epoch 3/15\n",
      "1977/1977 [==============================] - 2574s 1s/step - loss: 0.3586 - acc: 0.8620 - val_loss: 0.3572 - val_acc: 0.8626\n",
      "Epoch 4/15\n",
      "1977/1977 [==============================] - 2570s 1s/step - loss: 0.3450 - acc: 0.8670 - val_loss: 0.3475 - val_acc: 0.8660\n",
      "Epoch 5/15\n",
      "1977/1977 [==============================] - 2565s 1s/step - loss: 0.3335 - acc: 0.8710 - val_loss: 0.3412 - val_acc: 0.8686\n",
      "Epoch 6/15\n",
      "1977/1977 [==============================] - 2572s 1s/step - loss: 0.3249 - acc: 0.8740 - val_loss: 0.3370 - val_acc: 0.8703\n",
      "Epoch 7/15\n",
      "1977/1977 [==============================] - 2562s 1s/step - loss: 0.3183 - acc: 0.8763 - val_loss: 0.3307 - val_acc: 0.8719\n",
      "Epoch 8/15\n",
      "1977/1977 [==============================] - 2564s 1s/step - loss: 0.3129 - acc: 0.8783 - val_loss: 0.3280 - val_acc: 0.8732\n",
      "Epoch 9/15\n",
      "1977/1977 [==============================] - 2562s 1s/step - loss: 0.3083 - acc: 0.8798 - val_loss: 0.3256 - val_acc: 0.8743\n",
      "Epoch 10/15\n",
      "1977/1977 [==============================] - 2569s 1s/step - loss: 0.3043 - acc: 0.8814 - val_loss: 0.3233 - val_acc: 0.8749\n",
      "Epoch 11/15\n",
      "1977/1977 [==============================] - 2570s 1s/step - loss: 0.3007 - acc: 0.8826 - val_loss: 0.3214 - val_acc: 0.8754\n",
      "Epoch 12/15\n",
      "1977/1977 [==============================] - 2571s 1s/step - loss: 0.2975 - acc: 0.8838 - val_loss: 0.3194 - val_acc: 0.8766\n",
      "Epoch 13/15\n",
      "1977/1977 [==============================] - 2570s 1s/step - loss: 0.2948 - acc: 0.8848 - val_loss: 0.3193 - val_acc: 0.8766\n",
      "Epoch 14/15\n",
      "1977/1977 [==============================] - 2570s 1s/step - loss: 0.2919 - acc: 0.8857 - val_loss: 0.3180 - val_acc: 0.8761\n",
      "Epoch 15/15\n",
      "1977/1977 [==============================] - 2568s 1s/step - loss: 0.2897 - acc: 0.8864 - val_loss: 0.3165 - val_acc: 0.8773\n"
     ]
    }
   ],
   "source": [
    "dense_model = models.Sequential()\n",
    "dense_model.add(layers.Dense(832, input_shape=(832,), activation='relu'))\n",
    "dense_model.add(layers.Dense(64, activation='relu'))\n",
    "dense_model.add(layers.Dense(64, activation='relu'))\n",
    "dense_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "dense_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "dense_model.summary()\n",
    "\n",
    "# Fitting the model\n",
    "dense_history = dense_model.fit(x=train_dense_gen,\n",
    "                    validation_data=val_dense_gen,\n",
    "                    # steps_per_epoch=100,\n",
    "                    epochs=15,\n",
    "                    callbacks=[earlystop, tensorboard_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DenseModel-PB/assets\n"
     ]
    }
   ],
   "source": [
    "# dense_model.save('MLPmodel-Long-PB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially this model was only trained for 15 epochs, then when it appeared that it might be roughly comparable with the CNN model, it was trained for another 15 (early stopped after 11, 26 epochs in total) for a more direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1977/1977 [==============================] - 2595s 1s/step - loss: 0.2872 - acc: 0.8874 - val_loss: 0.3181 - val_acc: 0.8767\n",
      "Epoch 2/15\n",
      "1977/1977 [==============================] - 2590s 1s/step - loss: 0.2852 - acc: 0.8881 - val_loss: 0.3158 - val_acc: 0.8776ETA: 2:49 - l - ETA: 22s\n",
      "Epoch 3/15\n",
      "1977/1977 [==============================] - 2582s 1s/step - loss: 0.2833 - acc: 0.8888 - val_loss: 0.3150 - val_acc: 0.8778\n",
      "Epoch 4/15\n",
      "1977/1977 [==============================] - 2581s 1s/step - loss: 0.2816 - acc: 0.8895 - val_loss: 0.3151 - val_acc: 0.8781\n",
      "Epoch 5/15\n",
      "1977/1977 [==============================] - 2569s 1s/step - loss: 0.2800 - acc: 0.8899 - val_loss: 0.3153 - val_acc: 0.8774\n",
      "Epoch 6/15\n",
      "1977/1977 [==============================] - 2583s 1s/step - loss: 0.2784 - acc: 0.8904 - val_loss: 0.3144 - val_acc: 0.8785\n",
      "Epoch 7/15\n",
      "1977/1977 [==============================] - 2601s 1s/step - loss: 0.2770 - acc: 0.8909 - val_loss: 0.3159 - val_acc: 0.8780\n",
      "Epoch 8/15\n",
      "1977/1977 [==============================] - 2595s 1s/step - loss: 0.2756 - acc: 0.8914 - val_loss: 0.3156 - val_acc: 0.8782\n",
      "Epoch 9/15\n",
      "1977/1977 [==============================] - 2589s 1s/step - loss: 0.2743 - acc: 0.8919 - val_loss: 0.3155 - val_acc: 0.8783\n",
      "Epoch 10/15\n",
      "1977/1977 [==============================] - 2587s 1s/step - loss: 0.2731 - acc: 0.8923 - val_loss: 0.3175 - val_acc: 0.8785\n",
      "Epoch 11/15\n",
      "1977/1977 [==============================] - 2588s 1s/step - loss: 0.2721 - acc: 0.8926 - val_loss: 0.3160 - val_acc: 0.8781\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "dense_history = dense_model.fit(x=train_dense_gen,\n",
    "                    validation_data=val_dense_gen,\n",
    "                    epochs=15,\n",
    "                    callbacks=[earlystop, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_model.save('MLPmodel-Long.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autotuning\n",
    "\n",
    "For further information on the Keras autotuner, consult the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\"\n",
    "    Autotuner modeling function, based off the CNN model from the full-puzzle-model notebook.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Set Convolutional layer parameters\n",
    "    hp_filters = hp.Int('filters', min_value=16, max_value=128, step=8)\n",
    "    hp_ksize = hp.Int('kernel_size', min_value=2, max_value=8, step=2)\n",
    "\n",
    "    model.add(layers.Conv2D(filters=hp_filters, kernel_size=hp_ksize, padding='same', input_shape=(8,8,13), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2))\n",
    "    model.add(layers.Conv2D(filters=hp_filters, kernel_size=hp_ksize, padding='same', activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add dense tuning parameters\n",
    "    hp_units = hp.Int('units', min_value=16, max_value=128, step=8)\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder, objective='val_acc', max_epochs=10, directory='tuner', project_name='CNN_tuning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `model_builder` function and tuner created, let's start the search. The steps per epoch have been reduced in order to finish searching in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [02h 09m 36s]\n",
      "val_acc: 0.8775654435157776\n",
      "\n",
      "Best val_acc So Far: 0.8832182884216309\n",
      "Total elapsed time: 02h 04m 53s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "<keras_tuner.engine.hyperparameters.HyperParameters object at 0x7f4850260a90>\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=train_tune_gen, validation_data=val_tune_gen, steps_per_epoch=1000, callbacks=[earlystop])\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With parameters found, we can now train our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 8, 8, 120)         99960     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 120)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 120)         921720    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 96)                184416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 1,206,193\n",
      "Trainable params: 1,206,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "3953/3953 [==============================] - 2677s 677ms/step - loss: 0.2935 - acc: 0.8893 - val_loss: 0.2870 - val_acc: 0.8919\n",
      "Epoch 2/30\n",
      "3953/3953 [==============================] - 2696s 682ms/step - loss: 0.2733 - acc: 0.8959 - val_loss: 0.2728 - val_acc: 0.8965\n",
      "Epoch 3/30\n",
      "3953/3953 [==============================] - 2697s 682ms/step - loss: 0.2610 - acc: 0.9002 - val_loss: 0.2653 - val_acc: 0.8992\n",
      "Epoch 4/30\n",
      "3953/3953 [==============================] - 2693s 681ms/step - loss: 0.2524 - acc: 0.9032 - val_loss: 0.2617 - val_acc: 0.9006\n",
      "Epoch 5/30\n",
      "3953/3953 [==============================] - 2691s 681ms/step - loss: 0.2457 - acc: 0.9055 - val_loss: 0.2578 - val_acc: 0.9014\n",
      "Epoch 6/30\n",
      "3953/3953 [==============================] - 2684s 679ms/step - loss: 0.2399 - acc: 0.9074 - val_loss: 0.2564 - val_acc: 0.9021\n",
      "Epoch 7/30\n",
      "3953/3953 [==============================] - 2691s 681ms/step - loss: 0.2353 - acc: 0.9091 - val_loss: 0.2547 - val_acc: 0.9031\n",
      "Epoch 8/30\n",
      "3953/3953 [==============================] - 2691s 681ms/step - loss: 0.2310 - acc: 0.9106 - val_loss: 0.2568 - val_acc: 0.9032\n",
      "Epoch 9/30\n",
      "3953/3953 [==============================] - 2688s 680ms/step - loss: 0.2268 - acc: 0.9120 - val_loss: 0.2552 - val_acc: 0.9033\n",
      "Epoch 10/30\n",
      "3953/3953 [==============================] - 2685s 679ms/step - loss: 0.2233 - acc: 0.9131 - val_loss: 0.2560 - val_acc: 0.9034\n",
      "Epoch 11/30\n",
      "3953/3953 [==============================] - 2686s 680ms/step - loss: 0.2201 - acc: 0.9142 - val_loss: 0.2550 - val_acc: 0.9034\n",
      "Epoch 12/30\n",
      "3953/3953 [==============================] - 2668s 675ms/step - loss: 0.2174 - acc: 0.9151 - val_loss: 0.2561 - val_acc: 0.9033\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "tuned_model = tuner.get_best_models(num_models=1)[0]\n",
    "tuned_model.build()\n",
    "tuned_model.summary()\n",
    "\n",
    "tuned_history = tuned_model.fit(x=train_tune_gen,\n",
    "                    validation_data=val_tune_gen,\n",
    "                    epochs=30,\n",
    "                    callbacks=[earlystop, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tuned_model-PB/assets\n"
     ]
    }
   ],
   "source": [
    "# tuned_model.save(\"tuned_model-PB\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee0a5e5bf5625b85fe8b3503969a2ae7df2f5f7e92e10954b751982b3d5ddf25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('chess-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
